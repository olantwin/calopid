{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CNN for AdvSND target energy reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.losses\n",
    "import tensorflow.keras.metrics\n",
    "import tensorflow.keras.optimizers\n",
    "import uproot\n",
    "from iminuit import Minuit, cost\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D,\n",
    "    ReLU,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import plotting\n",
    "from CBAM3D import CBAM\n",
    "from preprocessing import reshape_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use([\"science\", \"notebook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.formatter.limits\"] = -5, 4\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"dataframe_CC_saturation5_train.root:df\"\n",
    "filename_test = \"dataframe_CC_saturation5_test.root:df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_train = uproot.open(filename_train)\n",
    "events_test = uproot.open(filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = events_train.num_entries + events_test.num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"lepton_energy\"\n",
    "\n",
    "target_pretty = \"Lepton Energy\"\n",
    "target_LaTeX = r\"E_\\ell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"hadron_energy\"\n",
    "\n",
    "target_pretty = \"Hadron Energy\"\n",
    "target_LaTeX = \"E_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"nu_energy\"\n",
    "\n",
    "target_pretty = \"Neutrino energy\"\n",
    "target_LaTeX = \"E_\\nu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"start_z\"\n",
    "\n",
    "target_pretty = \"Start Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"both\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"deps\"\n",
    "# edep_correction = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_generator(train=True):\n",
    "    events = events_train if train else events_test\n",
    "    log = \"energy\" in target\n",
    "    for batch, report in events.iterate(step_size=1, report=True, library=\"np\"):\n",
    "        for i in range(batch[\"X\"].shape[0]):\n",
    "            yield (\n",
    "                batch[\"X\"].astype(np.float16)[i],\n",
    "                batch[\"X_mufilter\"].astype(np.float16)[i],\n",
    "                (np.log(batch[target][i]) if log else batch[target][i]),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = event_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 3072, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(sample[0], aspect=0.05)\n",
    "plt.figure()\n",
    "plt.imshow(sample[1], aspect=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_spec_0 = tf.type_spec_from_value(gen.__next__()[0])\n",
    "generator_spec_1 = tf.type_spec_from_value(gen.__next__()[1])\n",
    "generator_spec_2 = tf.type_spec_from_value(gen.__next__()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reshape data only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        event_generator,\n",
    "        output_signature=(\n",
    "            generator_spec_0,\n",
    "            generator_spec_1,\n",
    "            generator_spec_2,\n",
    "        ),\n",
    "    )\n",
    "    .map(reshape_data)\n",
    "    .apply(tf.data.experimental.assert_cardinality(events_train.num_entries))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        event_generator,\n",
    "        args=[False],\n",
    "        output_signature=(\n",
    "            generator_spec_0,\n",
    "            generator_spec_1,\n",
    "            generator_spec_2,\n",
    "        ),\n",
    "    )\n",
    "    .map(reshape_data)\n",
    "    .apply(tf.data.experimental.assert_cardinality(events_test.num_entries))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = events_test[\"energy_dep_target\"].array() + edep_correction, events_test[\"energy_dep_mufilter\"].array()+edep_correction\n",
    "y_test = (\n",
    "    np.log(events_test[target].array())\n",
    "    if \"energy\" in target\n",
    "    else events_test[target].array()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ds_train = ds_train.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ds_test = ds_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "val_fraction = 0.2\n",
    "\n",
    "total_batches = tf.data.experimental.cardinality(batched_ds_train).numpy()\n",
    "train_size = int(total_batches * train_fraction)\n",
    "val_size = total_batches - train_size\n",
    "\n",
    "train_subset = batched_ds_train.take(train_size)\n",
    "val_subset = batched_ds_train.skip(train_size)\n",
    "\n",
    "# train_subset = train_subset.shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"CNN_3dSat5_grandjorasses_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    filters1 = hp.Int(\"filters1\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size1 = hp.Int(\"kernel_size1\", min_value=1, max_value=9, step=1)\n",
    "\n",
    "    filters2 = hp.Int(\"filters2\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size2 = hp.Int(\"kernel_size2\", min_value=1, max_value=9, step=1)\n",
    "\n",
    "    filters3 = hp.Int(\"filters3\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size3 = hp.Int(\"kernel_size3\", min_value=1, max_value=9, step=1)\n",
    "\n",
    "    filters4 = hp.Int(\"filters4\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size4 = hp.Int(\"kernel_size4\", min_value=1, max_value=9, step=1)\n",
    "\n",
    "    drop_rate = hp.Float(\"drop_rate\", min_value=0.1, max_value=0.6, step=0.1)\n",
    "\n",
    "    # Target Horizontal Branch\n",
    "    target_h_branch = Sequential(name=\"target_h_branch\")\n",
    "    target_h_branch.add(Input(shape=input_shape, name=\"target_h_in\"))\n",
    "\n",
    "    target_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_h1\"\n",
    "        )\n",
    "    )\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h1\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h1\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h1\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_h2\"\n",
    "        )\n",
    "    )\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h2\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h2\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h2\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_h3\"\n",
    "        )\n",
    "    )\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h3\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h3\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h3\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_h4\"\n",
    "        )\n",
    "    )\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h4\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h4\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_h4\"))\n",
    "\n",
    "    target_h_branch.add(Flatten(name=\"flatten_h\"))\n",
    "\n",
    "    # Target Vertical Branch\n",
    "    target_v_branch = Sequential(name=\"target_v_branch\")\n",
    "    target_v_branch.add(Input(shape=input_shape, name=\"target_v_in\"))\n",
    "\n",
    "    target_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_v1\"\n",
    "        )\n",
    "    )\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v1\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v1\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v1\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_v2\"\n",
    "        )\n",
    "    )\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v2\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v2\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v2\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_v3\"\n",
    "        )\n",
    "    )\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v3\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v3\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v3\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    target_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_v4\"\n",
    "        )\n",
    "    )\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v4\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v4\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_v4\"))\n",
    "\n",
    "    target_v_branch.add(Flatten(name=\"flatten_v\"))\n",
    "\n",
    "    # MU Filter Horizontal Branch\n",
    "    mufilter_h_branch = Sequential(name=\"mufilter_h_branch\")\n",
    "    mufilter_h_branch.add(Input(shape=(21, 4608, 1), name=\"mufilter_h_in\"))\n",
    "\n",
    "    mufilter_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_h1_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h1_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h1_1\"))\n",
    "    mufilter_h_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h1_1\")\n",
    "    )\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_h2_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h2_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h2_1\"))\n",
    "    mufilter_h_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h2_1\")\n",
    "    )\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_h3_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h3_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h3_1\"))\n",
    "    mufilter_h_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h3_1\")\n",
    "    )\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_h_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_h4_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h4_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h4_1\"))\n",
    "    mufilter_h_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_h4_1\")\n",
    "    )\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_h_branch.add(Flatten(name=\"flatten_h_1\"))\n",
    "\n",
    "    # MU Filter Vertical Branch\n",
    "    mufilter_v_branch = Sequential(name=\"mufilter_v_branch\")\n",
    "    mufilter_v_branch.add(Input(shape=(5, 4608, 1), name=\"mufilter_v_in\"))\n",
    "\n",
    "    mufilter_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_v1_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v1_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v1_1\"))\n",
    "    mufilter_v_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v1_1\")\n",
    "    )\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_v2_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v2_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v2_1\"))\n",
    "    mufilter_v_branch.add(\n",
    "        MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v2_1\")\n",
    "    )\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_v3_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v3_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v3_1\"))\n",
    "    mufilter_v_branch.add(\n",
    "        MaxPooling2D(pool_size=(1, 4), padding=\"valid\", name=\"pool_v3_1\")\n",
    "    )\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_v_branch.add(\n",
    "        Conv2D(\n",
    "            filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_v4_1\"\n",
    "        )\n",
    "    )\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v4_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v4_1\"))\n",
    "    mufilter_v_branch.add(\n",
    "        MaxPooling2D(pool_size=(1, 2), padding=\"same\", name=\"pool_v4_1\")\n",
    "    )\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "\n",
    "    mufilter_v_branch.add(Flatten(name=\"flatten_v_1\"))\n",
    "\n",
    "    X = Concatenate(name=\"concat_branches\")(\n",
    "        [\n",
    "            target_h_branch.output,\n",
    "            target_v_branch.output,\n",
    "            mufilter_v_branch.output,\n",
    "            mufilter_v_branch.output,\n",
    "        ]\n",
    "    )\n",
    "    X = Dense(4)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = Dense(20)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    X = Dense(1)(X)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[\n",
    "            target_h_branch.input,\n",
    "            target_v_branch.input,\n",
    "            mufilter_h_branch.input,\n",
    "            mufilter_v_branch.input,\n",
    "        ],\n",
    "        outputs=X,\n",
    "        name=model_name,\n",
    "    )\n",
    "\n",
    "    adam = Adam(\n",
    "        learning_rate=hp.Float(\n",
    "            \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n",
    "        )\n",
    "    )\n",
    "    model.compile(optimizer=adam, loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory=\"3D_hyperparam_opt\",\n",
    "    project_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train_subset,\n",
    "    validation_data=val_subset,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "toy_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(toy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO activation for max pooling?\n",
    "# TODO Reduce number of convolutional layers?\n",
    "# TODO Add hidden hidden layer (or two?) before outputs?\n",
    "# TODO predict independently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result = toy_model.fit(\n",
    "    batched_ds_train.prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.concat([history_df, pd.DataFrame(fit_result.history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.to_csv(f\"history_{model_name}_n{n_events}_e{len(history_df)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.save(f\"{model_name}_n{n_events}_e{len(history_df)}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "plt.title(\"CNN lepton + hadron energy\")\n",
    "ax1.plot(history_df[\"loss\"].values, color=colors[0])\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss Function\", color=colors[0])\n",
    "try:\n",
    "    ax2.plot(history_df[\"mae\"].values, color=colors[1])\n",
    "except KeyError:\n",
    "    ax2.plot(history_df[\"dense_2_mae\"].values, color=colors[1])\n",
    "    ax2.plot(history_df[\"dense_3_mae\"].values, color=colors[1])\n",
    "ax2.set_ylabel(\"Error\", color=colors[1])\n",
    "plt.text(\n",
    "    0.3,\n",
    "    0.7,\n",
    "    f\"Training dataset: {events_train.num_entries} events\\n\"\n",
    "    f\"Test dataset: {events_test.num_entries} events\\n\"\n",
    "    f\"Training duration: {len(history_df)} epochs\\n{model_name}\",\n",
    "    transform=ax1.transAxes,\n",
    ")\n",
    "plt.savefig(f\"plots/convergence_{model_name}_n{n_events}_e{len(history_df)}.pdf\")\n",
    "plt.savefig(f\"plots/convergence_{model_name}_n{n_events}_e{len(history_df)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test=retoy_model.predict(x=[x_test['scifi_h'], x_test['scifi_v'], x_test['us'], x_test['ds']])\n",
    "y_pred = toy_model.predict(batched_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = tensorflow.keras.metrics.RootMeanSquaredError()\n",
    "rms.update_state(y_test, y_pred)\n",
    "rmse_value = rms.result().numpy()\n",
    "print(\"Root Mean Squared Error:\", rmse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\"lepton_energy_pred\" : np.squeeze(np.exp(y_pred)[0]), \"lepton_energy_test\" : np.squeeze(np.exp(y_test)[0]),\n",
    "#                  \"hadron_energy_pred\" : np.squeeze(np.exp(y_pred)[1]), \"hadron_energy_test\" : np.squeeze(np.exp(y_test)[1])})\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        f\"{target}_pred\": np.squeeze(np.exp(y_pred)),\n",
    "        f\"{target}_test\": np.squeeze(np.exp(y_test)),\n",
    "    }\n",
    ")\n",
    "if \"energy\" not in target:\n",
    "    df = pd.DataFrame(\n",
    "        {f\"{target}_pred\": np.squeeze(y_pred), f\"{target}_test\": np.squeeze(y_test)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{model_name}_n{n_events}_e{len(history_df)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{model_name}_n{n_events}_e{len(history_df)}.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hist.Hist.new.Regular(20, -30, +30, name=r\"ùõ•z [cm]\").Double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.fill(np.squeeze(y_pred) - np.squeeze(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, mu, sigma):\n",
    "    return scipy.stats.norm.cdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries, edges = h.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(cost.BinnedNLL(entries, edges, model), 0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.plot()\n",
    "plt.xlabel(r\"$\\Delta z\\;[\\mathrm{cm}]$\")\n",
    "plt.title(\"3D CNN\")\n",
    "plot_range = edges[0], edges[-1]\n",
    "x = np.linspace(*plot_range, 100)\n",
    "best_fit = scipy.stats.norm(res.params[0].value, res.params[1].value)\n",
    "n_bins = len(entries)\n",
    "binsize = (plot_range[1] - plot_range[0]) / n_bins\n",
    "scale = h.sum() / (best_fit.cdf(plot_range[1]) - best_fit.cdf(plot_range[0])) * binsize\n",
    "plt.plot(x, scale * best_fit.pdf(x))\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.9,\n",
    "    rf\"$\\mu = {res.params[0].value:.2f} \\pm {res.params[0].error:.3f}$\\;cm\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.81,\n",
    "    rf\"$\\sigma = {res.params[1].value:.2f} \\pm {res.params[1].error:.3f}$\\;cm\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.text(\n",
    "    0.02,\n",
    "    0.78,\n",
    "    f\"Training dataset: {events_train.num_entries} events\\n\"\n",
    "    f\"Test dataset: {events_test.num_entries} events\\n\"\n",
    "    f\"Training duration: {len(history_df)} epochs\\n{model_name}\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "plotting.watermark()\n",
    "plt.savefig(f\"plots/h_dz_{model_name}_n{n_events}_e{len(history_df)}.pdf\")\n",
    "plt.savefig(f\"plots/h_dz_{model_name}_n{n_events}_e{len(history_df)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

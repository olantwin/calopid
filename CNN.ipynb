{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CNN for AdvSND target energy reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197610e-3702-4fbd-8f6c-05cb71eb4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import reshape_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032d727-4ead-4ba2-9488-4d673932e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb16a3c-c8a8-4d9d-8f73-833eea0a3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42277b0-762a-46fe-aa9a-77a2f0c2a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780232e-df08-4314-b888-702d34666226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87db5af-a91f-477d-a9fb-661ea81c7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import cost\n",
    "from iminuit import Minuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use([\"science\", \"notebook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.formatter.limits\"] = -5, 4\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"dataframe_CC_saturation5_train.root:df\"\n",
    "filename_test = \"dataframe_CC_saturation5_test.root:df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_train = uproot.open(filename_train)\n",
    "events_test = uproot.open(filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = events_train.num_entries + events_test.num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"lepton_energy\"\n",
    "\n",
    "target_pretty = \"Lepton Energy\"\n",
    "target_LaTeX = \"E_\\ell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"hadron_energy\"\n",
    "\n",
    "target_pretty = \"Hadron Energy\"\n",
    "target_LaTeX = \"E_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"nu_energy\"\n",
    "\n",
    "target_pretty = \"Neutrino energy\"\n",
    "target_LaTeX = \"E_\\nu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"start_z\"\n",
    "\n",
    "target_pretty = \"Start Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = \"both\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = \"deps\"\n",
    "#edep_correction = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_generator(train=True):\n",
    "    events = events_train if train else events_test\n",
    "    log = \"energy\" in target\n",
    "    for batch, report in events.iterate(step_size=1, report=True, library=\"np\"):\n",
    "        for i in range(batch[\"X\"].shape[0]):\n",
    "            yield (\n",
    "                batch[\"X\"].astype(np.float16)[i],\n",
    "                batch[\"X_mufilter\"].astype(np.float16)[i],\n",
    "                (np.log(batch[target][i]) if log else batch[target][i]),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = event_generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 3072, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(sample[0], aspect=0.05)\n",
    "plt.figure()\n",
    "plt.imshow(sample[1], aspect=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_spec_0 = tf.type_spec_from_value(gen.__next__()[0])\n",
    "generator_spec_1 = tf.type_spec_from_value(gen.__next__()[1])\n",
    "generator_spec_2 = tf.type_spec_from_value(gen.__next__()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reshape data only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        event_generator,\n",
    "        output_signature=(\n",
    "            generator_spec_0,\n",
    "            generator_spec_1,\n",
    "            generator_spec_2,\n",
    "        ),\n",
    "    )\n",
    "    .map(reshape_data)\n",
    "    .apply(tf.data.experimental.assert_cardinality(events_train.num_entries))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = (\n",
    "    tf.data.Dataset.from_generator(\n",
    "        event_generator,\n",
    "        args=[False],\n",
    "        output_signature=(\n",
    "            generator_spec_0,\n",
    "            generator_spec_1,\n",
    "            generator_spec_2,\n",
    "        ),\n",
    "    )\n",
    "    .map(reshape_data)\n",
    "    .apply(tf.data.experimental.assert_cardinality(events_test.num_entries))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = events_test[\"energy_dep_target\"].array() + edep_correction, events_test[\"energy_dep_mufilter\"].array()+edep_correction\n",
    "y_test = (\n",
    "    np.log(events_test[target].array())\n",
    "    if \"energy\" in target\n",
    "    else events_test[target].array()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ds_train = ds_train.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ds_test = ds_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252029b0-7e7d-4cd9-9e5b-103c446bbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "val_fraction = 0.2\n",
    "\n",
    "total_batches = tf.data.experimental.cardinality(batched_ds_train).numpy()\n",
    "train_size = int(total_batches * train_fraction)\n",
    "val_size = total_batches - train_size\n",
    "\n",
    "train_subset = batched_ds_train.take(train_size)\n",
    "val_subset = batched_ds_train.skip(train_size)\n",
    "\n",
    "#train_subset = train_subset.shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import (\n",
    "BatchNormalization,\n",
    "Concatenate,\n",
    "Conv2D,\n",
    "Dense,\n",
    "Dropout,\n",
    "Flatten,\n",
    "GlobalAveragePooling2D,\n",
    "Input,\n",
    "MaxPooling2D,\n",
    "Multiply,\n",
    "ReLU,\n",
    "Reshape\n",
    ")\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "import tensorflow.keras.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da129a1-a870-4cf0-96f3-c0123decedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CBAM3D import CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e24d8b-10b4-4ee1-94a7-cd022f06e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"CNN_3dSat5_grandjorasses_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d97c60-bbbe-4b8a-bef7-d61cb58297a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    filters1 = hp.Int(\"filters1\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size1 = hp.Int(\"kernel_size1\", min_value=1, max_value=9, step=1)\n",
    "    \n",
    "    filters2 = hp.Int(\"filters2\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size2 = hp.Int(\"kernel_size2\", min_value=1, max_value=9, step=1)\n",
    "    \n",
    "    filters3 = hp.Int(\"filters3\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size3 = hp.Int(\"kernel_size3\", min_value=1, max_value=9, step=1)\n",
    "    \n",
    "    filters4 = hp.Int(\"filters4\", min_value=16, max_value=64, step=16)\n",
    "    kernel_size4 = hp.Int(\"kernel_size4\", min_value=1, max_value=9, step=1)\n",
    "    \n",
    "    drop_rate = hp.Float(\"drop_rate\", min_value=0.1, max_value=0.6, step=0.1)\n",
    "    \n",
    "    # Target Horizontal Branch\n",
    "    target_h_branch = Sequential(name=\"target_h_branch\")\n",
    "    target_h_branch.add(Input(shape=input_shape, name=\"target_h_in\"))\n",
    "    \n",
    "    target_h_branch.add(Conv2D(filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_h1\"))\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h1\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h1\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h1\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_h_branch.add(Conv2D(filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_h2\"))\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h2\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h2\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h2\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_h_branch.add(Conv2D(filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_h3\"))\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h3\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h3\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h3\"))\n",
    "    target_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_h_branch.add(Conv2D(filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_h4\"))\n",
    "    target_h_branch.add(BatchNormalization(name=\"batch_norm_h4\"))\n",
    "    target_h_branch.add(ReLU())\n",
    "    target_h_branch.add(CBAM(name=\"CBAM_h4\"))\n",
    "    target_h_branch.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_h4\"))\n",
    "    \n",
    "    target_h_branch.add(Flatten(name=\"flatten_h\"))\n",
    "    \n",
    "    \n",
    "    # Target Vertical Branch\n",
    "    target_v_branch = Sequential(name=\"target_v_branch\")\n",
    "    target_v_branch.add(Input(shape=input_shape, name=\"target_v_in\"))\n",
    "    \n",
    "    target_v_branch.add(Conv2D(filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_v1\"))\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v1\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v1\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v1\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_v_branch.add(Conv2D(filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_v2\"))\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v2\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v2\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v2\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_v_branch.add(Conv2D(filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_v3\"))\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v3\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v3\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v3\"))\n",
    "    target_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    target_v_branch.add(Conv2D(filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_v4\"))\n",
    "    target_v_branch.add(BatchNormalization(name=\"batch_norm_v4\"))\n",
    "    target_v_branch.add(ReLU())\n",
    "    target_v_branch.add(CBAM(name=\"CBAM_v4\"))\n",
    "    target_v_branch.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_v4\"))\n",
    "    \n",
    "    target_v_branch.add(Flatten(name=\"flatten_v\"))\n",
    "    \n",
    "    \n",
    "    # MU Filter Horizontal Branch\n",
    "    mufilter_h_branch = Sequential(name=\"mufilter_h_branch\")\n",
    "    mufilter_h_branch.add(Input(shape=(21, 4608, 1), name=\"mufilter_h_in\"))\n",
    "    \n",
    "    mufilter_h_branch.add(Conv2D(filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_h1_1\"))\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h1_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h1_1\"))\n",
    "    mufilter_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h1_1\"))\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_h_branch.add(Conv2D(filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_h2_1\"))\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h2_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h2_1\"))\n",
    "    mufilter_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h2_1\"))\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_h_branch.add(Conv2D(filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_h3_1\"))\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h3_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h3_1\"))\n",
    "    mufilter_h_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_h3_1\"))\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_h_branch.add(Conv2D(filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_h4_1\"))\n",
    "    mufilter_h_branch.add(BatchNormalization(name=\"batch_norm_h4_1\"))\n",
    "    mufilter_h_branch.add(ReLU())\n",
    "    mufilter_h_branch.add(CBAM(name=\"CBAM_h4_1\"))\n",
    "    mufilter_h_branch.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"pool_h4_1\"))\n",
    "    mufilter_h_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_h_branch.add(Flatten(name=\"flatten_h_1\"))\n",
    "    \n",
    "    \n",
    "    # MU Filter Vertical Branch\n",
    "    mufilter_v_branch = Sequential(name=\"mufilter_v_branch\")\n",
    "    mufilter_v_branch.add(Input(shape=(5, 4608, 1), name=\"mufilter_v_in\"))\n",
    "    \n",
    "    mufilter_v_branch.add(Conv2D(filters=filters1, kernel_size=kernel_size1, padding=\"same\", name=\"conv_v1_1\"))\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v1_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v1_1\"))\n",
    "    mufilter_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v1_1\"))\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_v_branch.add(Conv2D(filters=filters2, kernel_size=kernel_size2, padding=\"same\", name=\"conv_v2_1\"))\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v2_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v2_1\"))\n",
    "    mufilter_v_branch.add(MaxPooling2D(pool_size=(2, 4), padding=\"valid\", name=\"pool_v2_1\"))\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_v_branch.add(Conv2D(filters=filters3, kernel_size=kernel_size3, padding=\"same\", name=\"conv_v3_1\"))\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v3_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v3_1\"))\n",
    "    mufilter_v_branch.add(MaxPooling2D(pool_size=(1, 4), padding=\"valid\", name=\"pool_v3_1\"))\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_v_branch.add(Conv2D(filters=filters4, kernel_size=kernel_size4, padding=\"same\", name=\"conv_v4_1\"))\n",
    "    mufilter_v_branch.add(BatchNormalization(name=\"batch_norm_v4_1\"))\n",
    "    mufilter_v_branch.add(ReLU())\n",
    "    mufilter_v_branch.add(CBAM(name=\"CBAM_v4_1\"))\n",
    "    mufilter_v_branch.add(MaxPooling2D(pool_size=(1, 2), padding=\"same\", name=\"pool_v4_1\"))\n",
    "    mufilter_v_branch.add(Dropout(rate=drop_rate))\n",
    "    \n",
    "    mufilter_v_branch.add(Flatten(name=\"flatten_v_1\"))\n",
    "\n",
    "\n",
    "    X = Concatenate(name=\"concat_branches\")(\n",
    "        [target_h_branch.output, target_v_branch.output, mufilter_v_branch.output, mufilter_v_branch.output])\n",
    "    X = Dense(4)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = Dense(20)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    X = Dense(1)(X)\n",
    "\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[target_h_branch.input, target_v_branch.input, mufilter_h_branch.input, mufilter_v_branch.input], outputs=X, name=model_name)\n",
    "    \n",
    "    adam = Adam(learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"))\n",
    "    model.compile(optimizer=adam, loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4caed-dd5d-431d-b950-3ce65f1716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory=\"3D_hyperparam_opt\",\n",
    "    project_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a764c7a-803d-4a25-bb7f-9ff968d99c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a897201-c380-4737-82c9-08fc65fd986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train_subset,\n",
    "    validation_data=val_subset,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dd69d-3a56-4fb1-a2ab-c2bcb88061d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3f05c-0b6e-4e36-b26f-fe3a53eea606",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "toy_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a67b6-479b-4f22-ac13-5076e9cb59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(toy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO activation for max pooling?\n",
    "# TODO Reduce number of convolutional layers?\n",
    "# TODO Add hidden hidden layer (or two?) before outputs?\n",
    "# TODO predict independently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result = toy_model.fit(\n",
    "    batched_ds_train.prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.concat([history_df, pd.DataFrame(fit_result.history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.to_csv(f\"history_{model_name}_n{n_events}_e{len(history_df)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.save(f\"{model_name}_n{n_events}_e{len(history_df)}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "plt.title(\"CNN lepton + hadron energy\")\n",
    "ax1.plot(history_df[\"loss\"].values, color=colors[0])\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss Function\", color=colors[0])\n",
    "try:\n",
    "    ax2.plot(history_df[\"mae\"].values, color=colors[1])\n",
    "except KeyError:\n",
    "    ax2.plot(history_df[\"dense_2_mae\"].values, color=colors[1])\n",
    "    ax2.plot(history_df[\"dense_3_mae\"].values, color=colors[1])\n",
    "ax2.set_ylabel(\"Error\", color=colors[1])\n",
    "plt.text(\n",
    "    0.3,\n",
    "    0.7,\n",
    "    f\"Training dataset: {events_train.num_entries} events\\n\"\n",
    "    f\"Test dataset: {events_test.num_entries} events\\n\"\n",
    "    f\"Training duration: {len(history_df)} epochs\\n{model_name}\",\n",
    "    transform=ax1.transAxes,\n",
    ")\n",
    "plt.savefig(f\"plots/convergence_{model_name}_n{n_events}_e{len(history_df)}.pdf\")\n",
    "plt.savefig(f\"plots/convergence_{model_name}_n{n_events}_e{len(history_df)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test=retoy_model.predict(x=[x_test['scifi_h'], x_test['scifi_v'], x_test['us'], x_test['ds']])\n",
    "y_pred = toy_model.predict(batched_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f3fd-7648-475f-aafa-8361ccb5523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = tensorflow.keras.metrics.RootMeanSquaredError()\n",
    "rms.update_state(y_test, y_pred)\n",
    "rmse_value = rms.result().numpy() \n",
    "print(\"Root Mean Squared Error:\", rmse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfcf58-4643-41ee-9227-0bdf4eb4abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.DataFrame({\"lepton_energy_pred\" : np.squeeze(np.exp(y_pred)[0]), \"lepton_energy_test\" : np.squeeze(np.exp(y_test)[0]),\n",
    "#                  \"hadron_energy_pred\" : np.squeeze(np.exp(y_pred)[1]), \"hadron_energy_test\" : np.squeeze(np.exp(y_test)[1])})\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        f\"{target}_pred\": np.squeeze(np.exp(y_pred)),\n",
    "        f\"{target}_test\": np.squeeze(np.exp(y_test)),\n",
    "    }\n",
    ")\n",
    "if \"energy\" not in target:\n",
    "    df = pd.DataFrame(\n",
    "        {f\"{target}_pred\": np.squeeze(y_pred), f\"{target}_test\": np.squeeze(y_test)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{model_name}_n{n_events}_e{len(history_df)}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{model_name}_n{n_events}_e{len(history_df)}.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hist.Hist.new.Regular(20, -30, +30, name=r\"𝛥z [cm]\").Double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da21ddf-b811-47ed-b4d3-0e6a22040153",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.fill(np.squeeze(y_pred) - np.squeeze(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea00d02-1374-433c-bf13-0eebafdfcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, mu, sigma):\n",
    "    return scipy.stats.norm.cdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6622d1-c20b-405e-a038-5efc4390f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries, edges = h.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147039e8-82bb-424f-823e-1815ad6cdad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(cost.BinnedNLL(entries, edges, model), 0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe8d53-1baf-49dd-8fb3-7d845c918d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906644e-27b7-419f-9a5b-880f0e7b957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.plot()\n",
    "plt.xlabel(r\"$\\Delta z\\;[\\mathrm{cm}]$\")\n",
    "plt.title(\"3D CNN\")\n",
    "plot_range = edges[0], edges[-1]\n",
    "x = np.linspace(*plot_range, 100)\n",
    "best_fit = scipy.stats.norm(res.params[0].value, res.params[1].value)\n",
    "n_bins = len(entries)\n",
    "binsize = (plot_range[1] - plot_range[0]) / n_bins\n",
    "scale = h.sum() / (best_fit.cdf(plot_range[1]) - best_fit.cdf(plot_range[0])) * binsize\n",
    "plt.plot(x, scale * best_fit.pdf(x))\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.9,\n",
    "    rf\"$\\mu = {res.params[0].value:.2f} \\pm {res.params[0].error:.3f}$\\;cm\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=11\n",
    ")\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.81,\n",
    "    rf\"$\\sigma = {res.params[1].value:.2f} \\pm {res.params[1].error:.3f}$\\;cm\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=11\n",
    ")\n",
    "plt.text(\n",
    "    0.02,\n",
    "    0.78,\n",
    "    f\"Training dataset: {events_train.num_entries} events\\n\"\n",
    "    f\"Test dataset: {events_test.num_entries} events\\n\"\n",
    "    f\"Training duration: {len(history_df)} epochs\\n{model_name}\",\n",
    "    transform=ax.transAxes,\n",
    "    usetex=True,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "plotting.watermark()\n",
    "plt.savefig(f\"plots/h_dz_{model_name}_n{n_events}_e{len(history_df)}.pdf\")\n",
    "plt.savefig(f\"plots/h_dz_{model_name}_n{n_events}_e{len(history_df)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fd093-a0b6-48e7-96b1-0cf428b71134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162f49b-1842-4f88-9653-78ef027333b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
